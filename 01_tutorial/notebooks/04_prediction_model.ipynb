{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70af7af2",
   "metadata": {},
   "source": [
    "# 時系列パート（基礎編：予測モデルの構築）\n",
    "\n",
    "ここでは，カフェの顧客データ（`cafe_customers.csv`）を使用して，時系列予測モデルの構築から評価まで実践的なスキルを学習します．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0843349e",
   "metadata": {},
   "source": [
    "## 必要なライブラリのインポート"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75f7b12",
   "metadata": {},
   "source": [
    "- **pandas, numpy**: データ処理の基本ライブラリ\n",
    "\n",
    "- **matplotlib**: グラフ作成ライブラリ\n",
    "\n",
    "- **sklearn**: 機械学習ライブラリ（Scikit-learn）\n",
    "  - `TimeSeriesSplit`: 時系列データ専用のクロスバリデーション\n",
    "  - `GridSearchCV`: ハイパーパラメータの自動調整\n",
    "  - 各種回帰アルゴリズム: LinearRegression, Ridge, RandomForest, GradientBoosting\n",
    "  - 評価指標: RMSE, MAE, R²など"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347789e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 警告メッセージを非表示にするライブラリ・設定\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 機械学習ライブラリ\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627dd0d9",
   "metadata": {},
   "source": [
    "## データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46015de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "feature_df = pd.read_csv('../data/processed/feature_engineered_data.csv')\n",
    "\n",
    "# Timestampをdatetime型に変換\n",
    "feature_df['Timestamp'] = pd.to_datetime(feature_df['Timestamp'])\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7dc12e",
   "metadata": {},
   "source": [
    "## 時系列データの適切な分割"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f5ab41",
   "metadata": {},
   "source": [
    "### なぜ時系列データは特別な分割が必要なのか？\n",
    "\n",
    "通常の機械学習では，データをランダムに訓練用・テスト用に分割します．\n",
    "\n",
    "しかし，時系列データでは以下の理由から**時間順序を保った分割**が必要です：\n",
    "\n",
    "1. **データリークの防止**: 未来の情報を使って過去を予測してしまう問題を避ける\n",
    "2. **現実的な評価**: 実際の運用では常に過去のデータで未来を予測する\n",
    "3. **時間依存性の考慮**: 時系列データには時間的な依存関係がある\n",
    "\n",
    "**分割方法**:\n",
    "- 訓練データ: 過去のデータ（データ全体の80%）\n",
    "- テストデータ: 最新のデータ（データ全体の20%）\n",
    "\n",
    "この方法により，モデルが実際の運用環境に近い条件で評価されます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef982ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータの日数を決定（データ全体の20%）\n",
    "total_days = (feature_df['Timestamp'].max() - feature_df['Timestamp'].min()).days\n",
    "test_days  = total_days // 5\n",
    "\n",
    "# 時系列データの適切な分割\n",
    "split_date  = feature_df['Timestamp'].max() - timedelta(days=test_days)\n",
    "trainval_df = feature_df[feature_df['Timestamp'] <= split_date].copy() # 訓練・検証データ\n",
    "test_df     = feature_df[feature_df['Timestamp'] > split_date].copy()  # テストデータ\n",
    "\n",
    "print(\"訓練・テストデータの期間\")\n",
    "print(f\"訓練期間  : {trainval_df['Timestamp'].min()} - {trainval_df['Timestamp'].max()}\")\n",
    "print(f\"テスト期間: {test_df['Timestamp'].min()} - {test_df['Timestamp'].max()}\")\n",
    "\n",
    "print(\"\\n訓練・テストデータの行数\")\n",
    "print(f\"訓練データ  : {len(trainval_df)} 行\")\n",
    "print(f\"テストデータ: {len(test_df)} 行\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cf91ad",
   "metadata": {},
   "source": [
    "### 説明変数と目的変数の分離\n",
    "\n",
    "機械学習では，データを以下のように分けて考えます：\n",
    "\n",
    "- **説明変数（X）**: 予測に使用する特徴量（時間，曜日，ラグ特徴量など）\n",
    "- **目的変数（y）**: 予測したい値（この場合は顧客数）\n",
    "\n",
    "**重要なポイント**:\n",
    "- `Timestamp`カラムは予測には直接使用しないため除外\n",
    "- `Customers`カラムが予測対象なので，これも説明変数から除外\n",
    "- これにより，モデルは時間情報と顧客数以外の全ての特徴量を学習に使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd14cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 説明変数Xと目的変数yを分離\n",
    "feature_cols = [col for col in feature_df.columns if col not in ['Timestamp', 'Customers']]\n",
    "\n",
    "X_train = trainval_df[feature_cols] # 訓練・検証データの説明変数\n",
    "y_train = trainval_df['Customers']  # 訓練・検証データの目的変数\n",
    "X_test = test_df[feature_cols]      # テストデータの説明変数\n",
    "y_test = test_df['Customers']       # テストデータの目的変数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cc4182",
   "metadata": {},
   "source": [
    "## モデルの構築と比較"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41806aa6",
   "metadata": {},
   "source": [
    "### 複数のアルゴリズムを比較する理由\n",
    "\n",
    "異なる機械学習アルゴリズムにはそれぞれ特徴があります：\n",
    "\n",
    "1. **線形回帰（Linear Regression）**\n",
    "   - 最もシンプルな手法\n",
    "   - 特徴量と目的変数の線形関係を学習\n",
    "   - 解釈しやすいが，複雑なパターンは捉えられない\n",
    "\n",
    "2. **Ridge回帰**\n",
    "   - 線形回帰に正則化を追加\n",
    "   - 過学習を防ぎ，汎化性能を向上\n",
    "   - 多重共線性の問題にも対処\n",
    "\n",
    "3. **ランダムフォレスト（Random Forest）**\n",
    "   - 複数の決定木を組み合わせた手法\n",
    "   - 非線形関係も学習可能\n",
    "   - 特徴量重要度が分析できる\n",
    "\n",
    "4. **勾配ブースティング（Gradient Boosting）**\n",
    "   - 弱学習器を順次改善していく手法\n",
    "   - 高い予測精度を実現\n",
    "   - ハイパーパラメータの調整が重要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8f1e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 複数のモデルを定義\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression' : Ridge(alpha=1.0),\n",
    "    'Random Forest'    : RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7a359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの訓練と評価\n",
    "results     = {} # 評価結果を格納する辞書\n",
    "predictions = {} # 予測結果を格納する辞書\n",
    "\n",
    "for name, model in tqdm(models.items()):\n",
    "    # モデル訓練\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 予測\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions[name] = y_pred\n",
    "    \n",
    "    # 評価指標計算（RMSE, MAE, R²）\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae  = mean_absolute_error(y_test, y_pred)\n",
    "    r2   = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {\n",
    "        'RMSE↓': rmse,\n",
    "        'MAE↓' : mae,\n",
    "        'R²↑'  : r2\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1d5cd3",
   "metadata": {},
   "source": [
    "### 評価指標の理解\n",
    "\n",
    "モデルの性能を測るため，以下の指標を使用します：\n",
    "\n",
    "1. **RMSE（Root Mean Squared Error）**\n",
    "   - 予測誤差の二乗平均の平方根\n",
    "   - 大きな誤差により敏感\n",
    "   - 単位は目的変数と同じ（この場合は「人」）\n",
    "   - **値が小さいほど良い** ↓\n",
    "\n",
    "2. **MAE（Mean Absolute Error）**\n",
    "   - 予測誤差の絶対値の平均\n",
    "   - 外れ値に対してRMSEより頑健\n",
    "   - 解釈しやすい（平均的な誤差）\n",
    "   - **値が小さいほど良い** ↓\n",
    "\n",
    "3. **R²（決定係数）**\n",
    "   - モデルがデータの分散をどれだけ説明できるかを示す\n",
    "   - 0〜1の値（1に近いほど良い予測）\n",
    "   - **値が大きいほど良い** ↑\n",
    "\n",
    "これらの指標を組み合わせることで，モデルの性能を多角的に評価できます．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82be7ff3",
   "metadata": {},
   "source": [
    "## 予測結果の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e18027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習結果の表示\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df.head().round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77dda2f",
   "metadata": {},
   "source": [
    "### 実際の値 vs 予測値で可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5ee4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Random Forest\"\n",
    "\n",
    "# 可視化（折れ線グラフ）\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(test_df['Timestamp'], y_test, label='Actual', linewidth=1, alpha=0.8)\n",
    "plt.plot(test_df['Timestamp'], predictions[model_name], label=f'{model_name} Prediction', linewidth=1, alpha=0.8)\n",
    "plt.title(f'Time Series Prediction: {model_name}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d0d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可視化（散布図）\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.scatter(y_test, predictions[model_name], alpha=0.6, s=20)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title(f'Actual vs Predicted: {model_name}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0433659",
   "metadata": {},
   "source": [
    "### 散布図（Actual vs Predicted）の見方\n",
    "\n",
    "**理想的な予測**では，すべての点が赤い対角線（y=x）上に乗ります．\n",
    "\n",
    "**散布図から分かること**：\n",
    "- **点が対角線に近い**: 予測精度が高い\n",
    "- **点が対角線より上**: 実際の値より低く予測している（過小評価）\n",
    "- **点が対角線より下**: 実際の値より高く予測している（過大評価）\n",
    "- **点のばらつき**: 予測の安定性（ばらつきが小さい方が良い）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b7ef79",
   "metadata": {},
   "source": [
    "### 残差分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d400487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 残差を計算（予測値 - 実測値）\n",
    "residuals = y_test - predictions[model_name]\n",
    "\n",
    "# 可視化（散布図）\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.scatter(predictions[model_name], residuals, alpha=0.6, s=20)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Predicted')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d066a0",
   "metadata": {},
   "source": [
    "### 残差分析とは\n",
    "\n",
    "**残差（Residuals）** = 実際の値 - 予測値\n",
    "\n",
    "残差分析は，モデルの予測誤差のパターンを調べる重要な手法です．\n",
    "\n",
    "**理想的な残差プロット**：\n",
    "- 残差が水平線（y=0）の周りにランダムに散らばっている\n",
    "- 特定のパターンや傾向が見られない\n",
    "- 分散が一定（等分散性）\n",
    "\n",
    "**問題のあるパターン**：\n",
    "- **曲線状のパターン**: モデルが非線形関係を捉えきれていない\n",
    "- **扇形のパターン**: 異分散性（予測値によって誤差の大きさが変わる）\n",
    "- **周期的なパターン**: 季節性や周期性を捉えきれていない"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3711df47",
   "metadata": {},
   "source": [
    "## ハイパーパラメータチューニング（グリッドサーチ）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3527247b",
   "metadata": {},
   "source": [
    "**ハイパーパラメータ**とは，学習前に設定する必要があるパラメータです（学習によって更新されるパラメータとは異なります）．\n",
    "\n",
    "**主要なハイパーパラメータ**：\n",
    "\n",
    "**Random Forest**:\n",
    "- `n_estimators`: 決定木の数（多いほど性能向上，計算時間増加）\n",
    "- `max_depth`: 木の最大深度（深いほど複雑なパターンを学習，過学習リスク増）\n",
    "- `min_samples_split`: 分割に必要な最小サンプル数（大きいほど過学習を防ぐ）\n",
    "\n",
    "**Gradient Boosting**:\n",
    "- `n_estimators`: ブースティング段階数\n",
    "- `learning_rate`: 学習率（小さいほど慎重に学習，計算時間増加）\n",
    "- `max_depth`: 各弱学習器の最大深度\n",
    "\n",
    "**Ridge Regression**:\n",
    "- `alpha`: 正則化の強さ（大きいほど過学習を防ぐ，小さいほど訓練データに適合）\n",
    "\n",
    "**チューニングの方法**：\n",
    "1. **グリッドサーチ**: 全ての組み合わせを試す（確実だが時間がかかる）\n",
    "2. **ランダムサーチ**: ランダムに組み合わせを試す（効率的）\n",
    "3. **ベイズ最適化**: より賢い探索方法（高度な手法）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633e4119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルごとのハイパーパラメータグリッド\n",
    "if model_name == 'Random Forest':\n",
    "    param_grid = {\n",
    "        'n_estimators'     : [50, 100, 200],\n",
    "        'max_depth'        : [None, 10, 20],\n",
    "        'min_samples_split': [2, 5]\n",
    "    }\n",
    "    base_model = RandomForestRegressor(random_state=42)\n",
    "    \n",
    "elif model_name == 'Gradient Boosting':\n",
    "    param_grid = {\n",
    "        'n_estimators' : [50, 100, 200],\n",
    "        'learning_rate': [0.05, 0.1, 0.2],\n",
    "        'max_depth'    : [3, 5, 7]\n",
    "    }\n",
    "    base_model = GradientBoostingRegressor(random_state=42)\n",
    "    \n",
    "elif model_name == 'Ridge Regression':\n",
    "    param_grid = {\n",
    "        'alpha': [0.1, 1.0, 10.0, 100.0]\n",
    "    }\n",
    "    base_model = Ridge()\n",
    "    \n",
    "else:  # Linear Regression\n",
    "    param_grid = {}\n",
    "    base_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5f2594",
   "metadata": {},
   "source": [
    "### 時系列クロスバリデーション（TimeSeriesSplit）\n",
    "\n",
    "通常のクロスバリデーションは，データをランダムに分割して交差検証を行います．しかし，時系列データでは**時間順序を保った分割**が必要です．\n",
    "\n",
    "**TimeSeriesSplitの仕組み**：\n",
    "```\n",
    "Split 1: Train [1,2,3] → Test [4]\n",
    "Split 2: Train [1,2,3,4] → Test [5]  \n",
    "Split 3: Train [1,2,3,4,5] → Test [6]\n",
    "```\n",
    "\n",
    "**特徴**：\n",
    "- 訓練データは常に過去，テストデータは未来\n",
    "- 各分割で訓練データが増加（より現実的）\n",
    "- データリークを完全に防ぐ\n",
    "\n",
    "**パラメータ**：\n",
    "- `n_splits=3`: 3回の交差検証を実行\n",
    "- 各分割で異なる時期のデータでモデルを評価\n",
    "- 最終的には全分割の平均スコアで最適パラメータを決定\n",
    "\n",
    "**注意点**：\n",
    "- 通常のKFoldより時間がかかる\n",
    "- データ量が少ないと訓練データが不足する可能性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60167145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 時系列クロスバリデーション\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# グリッドサーチ実行（数分かかります）\n",
    "grid_search = GridSearchCV(base_model, param_grid, cv=tscv, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14832989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最良パラメータで再予測\n",
    "best_tuned_model = grid_search.best_estimator_\n",
    "y_pred_tuned     = best_tuned_model.predict(X_test)\n",
    "\n",
    "# チューニング後の評価\n",
    "rmse_tuned    = np.sqrt(mean_squared_error(y_test, y_pred_tuned))\n",
    "rmse_original = results[model_name]['RMSE↓']\n",
    "\n",
    "print(f\"チューニング前RMSE: {rmse_original:.4f}\")\n",
    "print(f\"チューニング後RMSE: {rmse_tuned:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cdf1c3",
   "metadata": {},
   "source": [
    "## 特徴量重要度の分析（決定木ベースモデルのみ）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b2b303",
   "metadata": {},
   "source": [
    "### 特徴量重要度とは\n",
    "\n",
    "**特徴量重要度**は，各特徴量が予測にどれだけ貢献しているかを数値化したものです．\n",
    "\n",
    "**Random ForestとGradient Boostingの特徴量重要度**：\n",
    "- 各決定木における特徴量の分割回数と改善量に基づいて計算\n",
    "- 値が大きいほど，その特徴量が予測により重要\n",
    "- 全ての重要度の合計は1.0になる\n",
    "\n",
    "**活用方法**：\n",
    "1. **特徴選択**: 重要度の低い特徴量を除去してモデルを簡素化\n",
    "2. **ドメイン知識の検証**: 業務的に重要だと思われる特徴量が実際に重要か確認\n",
    "3. **新しい特徴量のヒント**: 重要な特徴量を参考に新しい特徴量を作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514a8097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量重要度を取得\n",
    "importances = best_tuned_model.feature_importances_\n",
    "\n",
    "# 重要度をDataFrameに整理\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature'   : feature_cols,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9705f296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可視化（全特徴量）\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.barh(range(len(feature_importance_df)), feature_importance_df['importance'])\n",
    "plt.yticks(range(len(feature_importance_df)), feature_importance_df['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title(f'Feature Importances - {model_name}')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a15e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可視化（重要度上位15特徴量）\n",
    "top_features = feature_importance_df.head(15)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title(f'Top 15 Feature Importances - {model_name}')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
